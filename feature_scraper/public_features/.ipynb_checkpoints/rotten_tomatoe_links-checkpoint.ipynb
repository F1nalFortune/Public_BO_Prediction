{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import math\n",
    "# import cv2\n",
    "import os\n",
    "import time\n",
    "from functools import wraps\n",
    "from colorama import Fore\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import ElementNotVisibleException\n",
    "from requests import ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(ExceptionToCheck, tries=4, delay=3, backoff=2, logger=None):\n",
    "    \"\"\"Retry calling the decorated function using an exponential backoff.\n",
    "\n",
    "    http://www.saltycrane.com/blog/2009/11/trying-out-retry-decorator-python/\n",
    "    original from: http://wiki.python.org/moin/PythonDecoratorLibrary#Retry\n",
    "\n",
    "    :param ExceptionToCheck: the exception to check. may be a tuple of\n",
    "        exceptions to check\n",
    "    :type ExceptionToCheck: Exception or tuple\n",
    "    :param tries: number of times to try (not retry) before giving up\n",
    "    :type tries: int\n",
    "    :param delay: initial delay between retries in seconds\n",
    "    :type delay: int\n",
    "    :param backoff: backoff multiplier e.g. value of 2 will double the delay\n",
    "        each retry\n",
    "    :type backoff: int\n",
    "    :param logger: logger to use. If None, print\n",
    "    :type logger: logging.Logger instance\n",
    "    \"\"\"\n",
    "    def deco_retry(f):\n",
    "\n",
    "        @wraps(f)\n",
    "        def f_retry(*args, **kwargs):\n",
    "            mtries, mdelay = tries, delay\n",
    "            while mtries > 1:\n",
    "                try:\n",
    "                    return f(*args, **kwargs)\n",
    "                except ExceptionToCheck:\n",
    "                    msg = \"%s, Retrying in %d seconds...\" % (str(ExceptionToCheck), mdelay)\n",
    "                    if logger:\n",
    "                        #logger.exception(msg) # would print stack trace\n",
    "                        logger.warning(msg)\n",
    "                    else:\n",
    "                        print(msg)\n",
    "                    time.sleep(mdelay)\n",
    "                    mtries -= 1\n",
    "                    mdelay *= backoff\n",
    "            return f(*args, **kwargs)\n",
    "\n",
    "        return f_retry  # true decorator\n",
    "\n",
    "    return deco_retry  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDailyDataset(text=False):\n",
    "  import pandas as pd\n",
    "  from tqdm import tqdm\n",
    "  import ast\n",
    "  import numpy as np\n",
    "\n",
    "  tqdm.pandas()\n",
    "  print(\"Reading data....\")\n",
    "  path = \"../../data(fixed)/\"\n",
    "\n",
    "  df = pd.read_csv(f\"{path}phelps_dataset(defense).csv\")\n",
    "  df['year'], df['month'], df['day'] = df['Year'], df['Month'], df['Day']\n",
    "  date_df = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "  df = pd.read_csv(f\"{path}phelps_dataset(defense).csv\")\n",
    "  df = df.drop(df.columns[0], axis=1)\n",
    "  df['date'] = date_df\n",
    "\n",
    "  df = df.sort_values('date', ascending=True).reset_index(drop=True)\n",
    "  print(len(df.loc[(df['summary']=='Add a Plot\\xa0»')]))\n",
    "  df = df.loc[(df['summary']!='Add a Plot\\xa0»')].reset_index(drop=True)\n",
    "\n",
    "  df = df.sort_values('date', ascending=True).reset_index(drop=True)\n",
    "  df = df.drop(columns=['max_theatres','plot', 'star_anchs'])\n",
    "\n",
    "  df = df.loc[(df['Year']>=2000)&(df['Year']<=2019)].reset_index(drop=True)\n",
    "\n",
    "  '''\n",
    "    CREATE CLASSIFICATION COLUMN\n",
    "  '''\n",
    "  y = []\n",
    "  '''\n",
    "    Less than 1m\n",
    "    1m - 20m\n",
    "    20m - 60m\n",
    "    60m - 200m\n",
    "    200m +\n",
    "  '''\n",
    "  for value in df['box_office']:\n",
    "    value = float(value)\n",
    "    if value < 1000000:\n",
    "      y.append(0)\n",
    "    elif value >= 1000000 and value < 20000000:\n",
    "      y.append(1)\n",
    "    elif value >= 20000000 and value < 60000000:\n",
    "      y.append(2)\n",
    "    elif value >= 60000000:\n",
    "      y.append(3)\n",
    "  df['classification'] = y\n",
    "  \n",
    "  for idx, row in enumerate(df.itertuples()):\n",
    "    classification = row.classification\n",
    "    if np.isnan(row.original_budget):\n",
    "      budget_average = df.loc[(df['classification']==classification)]['original_budget'].mean()\n",
    "#       df.set_value(idx, 'original_budget', budget_average)\n",
    "      df.at[idx, 'original_budget'] = budget_average\n",
    "    if np.isnan(row.original_theatres):\n",
    "      theatres_average = df.loc[(df['classification']==classification)]['original_theatres'].mean()\n",
    "#       df.set_value(idx, 'original_theatres', theatres_average)\n",
    "      df.at[idx, 'original_theatres']=theatres_average\n",
    "    \n",
    "  df['budget'], df['theatres'] = df['original_budget'], df['original_theatres']\n",
    "  df = df.drop(columns=['original_budget', 'original_theatres'])\n",
    "\n",
    "\n",
    "\n",
    "  if text==True:\n",
    "    text_columns = ['summary', 'genre_cat']\n",
    "    # re order to put categorical columns first\n",
    "    categories = ['holiday','mpaa','main_language','sequel_binary','main_prod','distributor','brand','main_genre','pre_release_screening']\n",
    "    ordered_columns =  text_columns + categories + [ elem for elem in df.columns if elem not in categories+text_columns] \n",
    "    df = df[ordered_columns]\n",
    "  #   breakpoint()\n",
    "    df['genre_cat'].fillna(\"['Unknown']\", inplace = True)\n",
    "    df['genre_cat'] = df['genre_cat'].apply(lambda x: ' '.join(ast.literal_eval(x)))\n",
    "  else:\n",
    "    df = df.drop(columns=['genre_cat', 'summary'])\n",
    "    # re order to put categorical columns first\n",
    "    categories = ['holiday','mpaa','main_language','sequel_binary','main_prod','distributor','brand','main_genre','pre_release_screening']\n",
    "    ordered_columns =  categories + [ elem for elem in df.columns if elem not in categories] \n",
    "    df = df[ordered_columns]\n",
    "    \n",
    "  df = df.dropna().reset_index(drop=True)\n",
    "  for val in df.columns:\n",
    "    try:\n",
    "      df[val] = df[val].astype(np.double)\n",
    "    except:\n",
    "      continue\n",
    "  for val in categories:\n",
    "    df[val] = df[val].astype('category')\n",
    "  df['classification'] = df['classification'].apply(lambda x: int(x))\n",
    "  print(\"\\n\")\n",
    "  print(\"Complete!\")\n",
    "\n",
    "\n",
    "  daily_df = pd.read_csv(f\"{path}daily_data.csv\", index_col=0)\n",
    "  daily_df = daily_df.drop(columns=['distributor'])\n",
    "\n",
    "  \n",
    "  df = pd.merge(df, daily_df, how='left', on=['name','profile'])\n",
    "\n",
    "  df = df.dropna(subset=['opening_weekend_gross'])\n",
    "  df = df.reset_index(drop=True)\n",
    "  \n",
    "  df = df.drop(columns=['main_language', 'main_prod', 'distributor', 'brand', 'main_genre'])\n",
    "  df['sequel_binary'], df['pre_release_screening'] = df['sequel_binary'].astype(int), df['pre_release_screening'].astype(int)\n",
    "\n",
    "  categories = ['holiday','mpaa']\n",
    "  df = pd.get_dummies(df, columns=categories)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianphelps/opt/anaconda3/envs/thesis/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data....\n",
      "29\n",
      "\n",
      "\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "df = createDailyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'] = 'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./url_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleSpider(object):\n",
    "  def __init__(self, header):\n",
    "    \"\"\"\n",
    "      Crawl Google search results\n",
    "\n",
    "      This class is used to crawl Google's search results using requests and BeautifulSoup.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.headers = {\n",
    "      'User-Agent': header,\n",
    "      'Host': 'www.google.com',\n",
    "      'Referer': 'https://www.google.com/'\n",
    "    }\n",
    "\n",
    "  def __get_source(self, url: str) -> requests.Response:\n",
    "    \"\"\"\n",
    "      Get the web page's source code\n",
    "\n",
    "      Args:\n",
    "        url (str): The URL to crawl\n",
    "\n",
    "      Returns:\n",
    "        requests.Response: The response from URL\n",
    "    \"\"\"\n",
    "    return requests.get(url, headers=self.headers)\n",
    "\n",
    "  def search(self, query: str) -> list:\n",
    "      \"\"\"\n",
    "        Search Google\n",
    "\n",
    "        Args:\n",
    "          query (str): The query to search for\n",
    "\n",
    "        Returns:\n",
    "          list: The search results\n",
    "      \"\"\"\n",
    "      # Get response\n",
    "      response = self.__get_source(f'https://www.google.com/{query}')\n",
    "      # Initialize BeautifulSoup\n",
    "      soup = BeautifulSoup(response.text, 'html.parser')\n",
    "      # Get the result containers\n",
    "      result_containers = soup.findAll('a')\n",
    "      # Final results list\n",
    "      results = []\n",
    "      for anchor in result_containers:\n",
    "        try:\n",
    "          href = anchor['href']\n",
    "          if 'rottentomatoes.com/m/' in href:\n",
    "            url = href\n",
    "            return url\n",
    "        except:\n",
    "          pass\n",
    "      return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "# Import dependencies\n",
    "from urllib.parse import quote\n",
    "from pprint import pprint\n",
    "from googlesearch import search \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.nan, 'nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_path = \"/Users/brianphelps/Desktop/chromedriver\"\n",
    "\n",
    "# OPTIONS\n",
    "\n",
    "# options.add_argument(\"headless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for idx, row in enumerate(df.itertuples()):\n",
    "  if row.url=='nan':\n",
    "    indexes.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(indexes[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019: Before You Know It\n",
      "https://www.rottentomatoes.com/m/before_you_know_it_2019\n",
      "43/3545\n"
     ]
    }
   ],
   "source": [
    "result_dataframe = pd.DataFrame()\n",
    "\n",
    "API_KEY = '8ed1c1a9d314e36eb0baf42b055e46e6'\n",
    "counter=0\n",
    "user_agent = UserAgent()\n",
    "for row in df.itertuples():\n",
    "  if row.url=='nan':\n",
    "    time.sleep(2)\n",
    "    counter+=1\n",
    "    name = urllib.parse.quote(row.name)\n",
    "    year = int(row.Year)\n",
    "    profile = row.profile\n",
    "    \n",
    "    headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:79.0) Gecko/20100101 Firefox/79.0',\n",
    "            'Host': 'www.google.com',\n",
    "            'Referer': 'https://www.google.com/'\n",
    "        }\n",
    "    \n",
    "    query = f\"/search?q={name}%20{year}%20site:rottentomatoes.com\"\n",
    "    user_data_path = f\"/Users/brianphelps/Library/Application Support/Google/Chrome/Defaultzorro{counter+9000}\"\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"no-sandbox\")\n",
    "    options.add_argument(\"disable-dev-shm-usage\")\n",
    "    options.add_argument(f\"user-data-dir={user_data_path}\")\n",
    "#     if counter > 3:\n",
    "#       options.add_argument(\"headless\")\n",
    "    \n",
    "    \n",
    "    driver = webdriver.Chrome(f\"{chromedriver_path}\", chrome_options=options)\n",
    "\n",
    "    @retry((ConnectionResetError,TimeoutException,ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError), tries=20, delay=2, backoff=2)\n",
    "    def driver_reviews():\n",
    "      driver.get(f\"https://www.google.com{query}\")\n",
    "    driver_reviews()\n",
    "\n",
    "    src = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    # Get the result containers\n",
    "    result_containers = soup.findAll('a')\n",
    "    # Final results list\n",
    "    results = []\n",
    "    search_anchors=True\n",
    "    for anchor in result_containers:\n",
    "      if search_anchors:\n",
    "        try:\n",
    "          href = anchor['href']\n",
    "          if 'rottentomatoes.com/m/' in href:\n",
    "            url = href\n",
    "            print(f\"{year}: {row.name}\")\n",
    "            print(url)\n",
    "            print(f\"{counter}/{len(df)}\")\n",
    "            clear_output(wait=True)\n",
    "            temp = pd.DataFrame({\n",
    "              'profile': [profile],\n",
    "              'url': [url]\n",
    "            })\n",
    "            result_dataframe = result_dataframe.append(temp, ignore_index=True)\n",
    "            search_anchors=False\n",
    "            continue\n",
    "        except:\n",
    "          url = np.nan\n",
    "          print(f\"{year}: {row.name}\")\n",
    "          print(url)\n",
    "          print(f\"{counter}/{len(df)}\")\n",
    "          clear_output(wait=True)\n",
    "          temp = pd.DataFrame({\n",
    "            'profile': [profile],\n",
    "            'url': [url]\n",
    "          })\n",
    "          result_dataframe = result_dataframe.append(temp, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 2)\n"
     ]
    }
   ],
   "source": [
    "result_dataframe = result_dataframe.dropna()\n",
    "result_dataframe = result_dataframe.reset_index(drop=True)\n",
    "print(result_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in result_dataframe.itertuples():\n",
    "  profile = row.profile\n",
    "  index = df.loc[df['profile']==profile].index[0]\n",
    "  try:\n",
    "    df.at[index, 'url']=row.url\n",
    "  except:\n",
    "    breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3545"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[(df['url']!='nan')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"url_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       https://www.rottentomatoes.com/m/snow_falling_...\n",
       "1               https://www.rottentomatoes.com/m/magnolia\n",
       "2            https://www.rottentomatoes.com/m/next_friday\n",
       "3       https://www.rottentomatoes.com/m/girl_interrupted\n",
       "4       https://www.rottentomatoes.com/m/1093641-hurri...\n",
       "                              ...                        \n",
       "3540               https://www.rottentomatoes.com/m/saaho\n",
       "3541          https://www.rottentomatoes.com/m/todos_caen\n",
       "3542    https://www.rottentomatoes.com/m/dont_let_go_2019\n",
       "3543        https://www.rottentomatoes.com/m/bennetts_war\n",
       "3544    https://www.rottentomatoes.com/m/before_you_kn...\n",
       "Name: url, Length: 3545, dtype: object"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = df[['name', 'profile', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "links.to_csv(\"./rotten_tomatoe_links.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
