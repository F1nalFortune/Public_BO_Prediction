{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import math\n",
    "# import cv2\n",
    "import os\n",
    "import time\n",
    "from functools import wraps\n",
    "from colorama import Fore\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import ElementNotVisibleException\n",
    "from requests import ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "# Import dependencies\n",
    "from urllib.parse import quote\n",
    "from pprint import pprint\n",
    "from googlesearch import search \n",
    "import random\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(ExceptionToCheck, tries=4, delay=3, backoff=2, logger=None):\n",
    "    \"\"\"Retry calling the decorated function using an exponential backoff.\n",
    "\n",
    "    http://www.saltycrane.com/blog/2009/11/trying-out-retry-decorator-python/\n",
    "    original from: http://wiki.python.org/moin/PythonDecoratorLibrary#Retry\n",
    "\n",
    "    :param ExceptionToCheck: the exception to check. may be a tuple of\n",
    "        exceptions to check\n",
    "    :type ExceptionToCheck: Exception or tuple\n",
    "    :param tries: number of times to try (not retry) before giving up\n",
    "    :type tries: int\n",
    "    :param delay: initial delay between retries in seconds\n",
    "    :type delay: int\n",
    "    :param backoff: backoff multiplier e.g. value of 2 will double the delay\n",
    "        each retry\n",
    "    :type backoff: int\n",
    "    :param logger: logger to use. If None, print\n",
    "    :type logger: logging.Logger instance\n",
    "    \"\"\"\n",
    "    def deco_retry(f):\n",
    "\n",
    "        @wraps(f)\n",
    "        def f_retry(*args, **kwargs):\n",
    "            mtries, mdelay = tries, delay\n",
    "            while mtries > 1:\n",
    "                try:\n",
    "                    return f(*args, **kwargs)\n",
    "                except ExceptionToCheck:\n",
    "                    msg = \"%s, Retrying in %d seconds...\" % (str(ExceptionToCheck), mdelay)\n",
    "                    if logger:\n",
    "                        #logger.exception(msg) # would print stack trace\n",
    "                        logger.warning(msg)\n",
    "                    else:\n",
    "                        print(msg)\n",
    "                    time.sleep(mdelay)\n",
    "                    mtries -= 1\n",
    "                    mdelay *= backoff\n",
    "            return f(*args, **kwargs)\n",
    "\n",
    "        return f_retry  # true decorator\n",
    "\n",
    "    return deco_retry  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data(fixed)/datasets/rotten_tomatoe_df.csv\",index_col=0)\n",
    "df = df.drop(df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "one,two,three,four,five,six,seven = df[:500],df[500:1000], df[1000:1500],df[1500:2000],df[2000:2500], df[2500:3000], df[3000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selenium Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOWS\n",
    "# chromedriver_path = str(\"\"\"C:/Users/Brian/Desktop/chromedriver.exe\"\"\")\n",
    "# user_data_path = \"C:/Users/Brian/AppData/Local/Google/Chrome/User Data/Thread_Five\"\n",
    "\n",
    "# MAC\n",
    "chromedriver_path = \"/Users/brianphelps/Desktop/chromedriver\"\n",
    "user_data_path = \"/Users/brianphelps/Library/Application Support/Google/Chrome/Thread_SWOOPS\"\n",
    "\n",
    "# OPTIONS\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"no-sandbox\")\n",
    "options.add_argument(\"disable-dev-shm-usage\")\n",
    "options.add_argument(\"headless\")\n",
    "# options.add_argument(f\"user-data-dir={user_data_path}\")\n",
    "# driver = webdriver.Chrome(f\"{chromedriver_path}\", chrome_options=options)\n",
    "# driver.get('https://pro.imdb.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RottenTomatoeSpider(object):\n",
    "  def __init__(self, df):\n",
    "    \"\"\"Crawl Rotten Tomatoe search results\n",
    "\n",
    "    This class is used to crawl Rotten Tomatoe's reviews using Selenium and BeautifulSoup.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    chromedriver_path = \"/Users/brianphelps/Desktop/chromedriver\"\n",
    "    user_data_path = \"/Users/brianphelps/Library/Application Support/Google/Chrome/Thread_SWOOPS\"\n",
    "\n",
    "    # OPTIONS\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"no-sandbox\")\n",
    "    options.add_argument(\"disable-dev-shm-usage\")\n",
    "    options.add_argument(\"headless\")\n",
    "#     options.add_argument(f\"user-data-dir={user_data_path}\")\n",
    "    \n",
    "    self.options = options\n",
    "    self.chromedriver_path = chromedriver_path\n",
    "    self.data = df\n",
    "    self.review_dataframe = pd.DataFrame()\n",
    "\n",
    "  def critic_reviews(self, thread_name):\n",
    "    \"\"\"Get the web page's source code\n",
    "\n",
    "    Args:\n",
    "        thread_name (str): Filename of thread to save\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Critic review results\n",
    "    \"\"\"\n",
    "    counter=0\n",
    "    for row in self.data.itertuples():\n",
    "      counter+=1\n",
    "      profile = row.profile\n",
    "      url = row.url\n",
    "      name = row.name\n",
    "      year = int(row.Year)\n",
    "      '''\n",
    "          GET REVIEWS\n",
    "      '''\n",
    "\n",
    "      driver = webdriver.Chrome(f\"{self.chromedriver_path}\", chrome_options=self.options)\n",
    "\n",
    "      @retry((ConnectionResetError,TimeoutException,ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError), tries=20, delay=2, backoff=2)\n",
    "      def driver_reviews(url):\n",
    "        driver.get(f\"{url}\")\n",
    "      driver_reviews(url+'/reviews')\n",
    "\n",
    "      src = driver.page_source\n",
    "      parser = BeautifulSoup(src, 'lxml')\n",
    "      base_url = 'https://www.rottentomatoes.com/'\n",
    "\n",
    "      try:\n",
    "        num_pages = int(parser.find('span', class_='pageInfo').text.replace(\"Page 1 of \", \"\").strip())\n",
    "      except:\n",
    "        num_pages = 1\n",
    "\n",
    "\n",
    "      review_count=0\n",
    "      for i in range(num_pages):\n",
    "        src = driver.page_source\n",
    "        parser = BeautifulSoup(src, 'lxml')\n",
    "        try:\n",
    "          next_link_one = parser.find('span', class_='pageInfo').parent.find_all('a')[1]['href']\n",
    "          next_link = base_url+next_link_one\n",
    "        except:\n",
    "          next_link_one = 'nan'\n",
    "          next_link = np.nan\n",
    "\n",
    "        try:\n",
    "          reviews_container = parser.find('div', class_='review_table')\n",
    "          review_containers = reviews_container.find_all('div', class_='review_table_row')\n",
    "        except:\n",
    "          driver.quit()\n",
    "          break\n",
    "\n",
    "        movie_reviews = pd.DataFrame()\n",
    "\n",
    "        for review in review_containers:\n",
    "          # FIND SENTIMENT\n",
    "          fresh = review.find('div', class_='fresh')\n",
    "          if fresh:\n",
    "            sentiment = 'fresh'\n",
    "          else:\n",
    "            sentiment = 'rotten'\n",
    "              \n",
    "          # TOP CRITIC BOOLEAN\n",
    "          top = review.find('span', class_='glyphicon-star')\n",
    "          if top:\n",
    "            top_critic = True\n",
    "          else:\n",
    "            top_critic = False\n",
    "\n",
    "          try:\n",
    "            date = review.find('div', class_='review-date').text.strip()\n",
    "          except:\n",
    "            date = np.nan\n",
    "            \n",
    "          try:\n",
    "            the_review = review.find('div', class_='the_review').text.strip()\n",
    "          except:\n",
    "            the_review = np.nan\n",
    "\n",
    "          temp = pd.DataFrame({\n",
    "            'date': [date],\n",
    "            'review': [the_review],\n",
    "            'sentiment': [sentiment],\n",
    "            'movie': [name],\n",
    "            'profile': [profile],\n",
    "            'top_critic': [top_critic]\n",
    "          })\n",
    "          review_count+=1\n",
    "          self.review_dataframe = self.review_dataframe.append(temp, ignore_index=True)\n",
    "          print(Fore.GREEN + name)\n",
    "          print(Fore.GREEN + f\"Reviews: {review_count}\")\n",
    "          print(Fore.GREEN + f\"{counter}/{len(self.data)}\")\n",
    "          clear_output(wait = True)\n",
    "          \n",
    "          \n",
    "          \n",
    "#         self.review_dataframe.to_csv(f\"./rotten_tomatoe_critic_reviews_{thread_name}.csv\", index=False)\n",
    "        if next_link_one not in [\"#\",\"nan\"] and type(next_link)==str:\n",
    "          driver_reviews(next_link)\n",
    "        else:\n",
    "          driver.quit()\n",
    "          \n",
    "    return self.review_dataframe\n",
    "\n",
    "  def audience_reviews(self):\n",
    "    \"\"\"Search Rotten Tomatoe Audience Reviews\n",
    "\n",
    "    Returns:\n",
    "        dataframe: Audience Reviews\n",
    "    \"\"\"\n",
    "\n",
    "    counter=0\n",
    "    for row in self.data.itertuples():\n",
    "      counter+=1\n",
    "      profile = row.profile\n",
    "      url = row.url\n",
    "      name = row.name\n",
    "      year = int(row.Year)\n",
    "      '''\n",
    "          GET REVIEWS\n",
    "      '''\n",
    "\n",
    "      driver = webdriver.Chrome(f\"{self.chromedriver_path}\", chrome_options=self.options)\n",
    "\n",
    "      @retry((ConnectionResetError,TimeoutException,ReadTimeout, ConnectTimeout, HTTPError, Timeout, ConnectionError), tries=20, delay=2, backoff=2)\n",
    "      def driver_reviews(url):\n",
    "        driver.get(f\"{url}\")\n",
    "      driver_reviews(url+'/reviews?type=user')\n",
    "\n",
    "      src = driver.page_source\n",
    "      parser = BeautifulSoup(src, 'lxml')\n",
    "      base_url = 'https://www.rottentomatoes.com/'\n",
    "\n",
    "\n",
    "      current_movie_reviews = pd.DataFrame()\n",
    "      review_count=0\n",
    "\n",
    "      next_page_btn = True\n",
    "\n",
    "      while next_page_btn == True:\n",
    "\n",
    "        src = driver.page_source\n",
    "        parser = BeautifulSoup(src, 'lxml')\n",
    "        for button in parser.find_all(\"button\", class_='hide'): \n",
    "          #delete all hidden buttons\n",
    "          button.decompose()\n",
    "\n",
    "        try:\n",
    "          #Delete Element From DOM\n",
    "          #PYTHON\n",
    "          for button in parser.find_all(\"button\", class_='hide'): \n",
    "            #delete all hidden buttons\n",
    "            button.decompose()\n",
    "          #JS\n",
    "          driver.execute_script(\"\"\"\n",
    "            var elements = document.getElementsByClassName('prev-next-paging__button-right hide');\n",
    "            for(i=0;i<elements.length;i++){\n",
    "              elements[i].parentNode.removeChild(elements[i]);\n",
    "            }\n",
    "          \"\"\")\n",
    "          next_link = parser.find('button', class_='prev-next-paging__button-right')\n",
    "        except:\n",
    "          next_link = 'nan'\n",
    "          next_page_btn = False\n",
    "\n",
    "        try:\n",
    "          reviews_container = parser.find('ul', class_='audience-reviews')\n",
    "          review_containers = reviews_container.find_all('li', class_='audience-reviews__item')\n",
    "        except:\n",
    "          driver.quit()\n",
    "          break\n",
    "\n",
    "        for review in review_containers:\n",
    "\n",
    "          full = len(review.find_all('span', class_='star-display__filled'))\n",
    "          half = len(review.find_all('span', class_='star-display__half'))\n",
    "          sentiment = 0\n",
    "          if full:\n",
    "            for i in range(full):\n",
    "              sentiment+=1\n",
    "          if half:\n",
    "            for i in range(half):\n",
    "              sentiment+=.5\n",
    "\n",
    "\n",
    "          try:\n",
    "            date = review.find('span', class_='audience-reviews__duration').text.strip()\n",
    "          except:\n",
    "            date = np.nan\n",
    "          try:\n",
    "            the_review = review.find('p').text.strip()\n",
    "          except:\n",
    "            the_review = np.nan\n",
    "\n",
    "          temp = pd.DataFrame({\n",
    "            'date': [date],\n",
    "            'review': [the_review],\n",
    "            'sentiment': [sentiment],\n",
    "            'movie': [name],\n",
    "            'profile': [profile]\n",
    "          })\n",
    "          review_count+=1\n",
    "\n",
    "          current_movie_reviews = current_movie_reviews.append(temp, ignore_index=True)\n",
    "          print(Fore.GREEN + name)\n",
    "          print(Fore.GREEN + f\"Reviews: {review_count}\")\n",
    "          print(Fore.GREEN + f\"{counter}/{len(self.data)}\")\n",
    "          clear_output(wait = True)\n",
    "        if next_link!='nan':\n",
    "          try:\n",
    "            driver.execute_script(\"\"\"\n",
    "              var element = document.getElementsByClassName('prev-next-paging__button-right')[0];\n",
    "              if(element.classList.contains('hide')){\n",
    "\n",
    "              }else{\n",
    "                element.click()\n",
    "              }\n",
    "            \"\"\")\n",
    "            time.sleep(1)\n",
    "          except:\n",
    "            next_page_btn = False\n",
    "#         self.review_dataframe = self.review_dataframe.append(current_movie_reviews, ignore_index=True).drop_duplicates()\n",
    "#         self.review_dataframe.to_csv(f\"./rotten_tomatoe_audience_reviews_{thread_name}.csv\", index=False)\n",
    "    return self.review_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mDreamgirls\n",
      "\u001b[32mReviews: 208\n",
      "\u001b[32m480/480\n"
     ]
    }
   ],
   "source": [
    "two = two.reset_index(drop=True)\n",
    "saved_dataframe = spider.review_dataframe.copy()\n",
    "two = two.iloc[20:]\n",
    "\n",
    "spider = RottenTomatoeSpider(two)\n",
    "two_reviews = spider.critic_reviews('two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_reviews = two_reviews.append(saved_dataframe, ignore_index=True)\n",
    "two_reviews.to_csv(\"rotten_tomatoe_critic_reviews_two.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
