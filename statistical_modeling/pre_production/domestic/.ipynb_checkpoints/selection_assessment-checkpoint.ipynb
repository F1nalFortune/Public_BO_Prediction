{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_val(df,model,space, n_iter,scoring,search_cv='random',num_classes=4,outer=10,inner=3,seed=42):\n",
    "  \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Creates nested folds manually.\n",
    "    Returns dataframe  of validation data\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "      Input ready in form of dataframe.\n",
    "        \n",
    "    model : machine learning model\n",
    "      Model in which to cross validate.\n",
    "      eg) CatboostRegressor, LightGBMRegressor\n",
    "      \n",
    "    space : dictionary of lists\n",
    "      Search space for hyper parameter optimization.\n",
    "      eg) {\n",
    "            'num_leaves': [300,400,500], \n",
    "            'max_bin': [175,255,510],\n",
    "            'num_iterations': [700,800,900], \n",
    "            'learning_rate':[0.05,0.1,0.15],\n",
    "            'boosting_type': ['gbdt', 'dart'],\n",
    "            'max_depth': [-1, 5, 10]\n",
    "          }\n",
    "      \n",
    "    n_iter : number of parameter settings sampled\n",
    "      n_iter trades off runtime vs quality of the solution.\n",
    "      \n",
    "    scoring : string\n",
    "      score method for evaluating model\n",
    "      \n",
    "    search_cv : string, default='random'\n",
    "      hyper-parameter optimization approach. Random and grid search options\n",
    "      eg)'random' or 'grid'\n",
    "      \n",
    "    num_classes : int, default=4\n",
    "      number of classes for classification.\n",
    "      eg) 4, 6, 8\n",
    "      \n",
    "    outer : int, default=10\n",
    "      number of outer folds\n",
    "      \n",
    "    inner : int, default=3\n",
    "      number of inner folds\n",
    "      \n",
    "    seed : int, default=42\n",
    "      seed for reproducability\n",
    "      \n",
    "      \n",
    "    Returns\n",
    "    ----------\n",
    "    Dataframe with final results.\n",
    "    \"\"\"\n",
    "  from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "  from sklearn.metrics import balanced_accuracy_score\n",
    "  from sklearn.preprocessing import label_binarize\n",
    "  from sklearn.multiclass import OneVsRestClassifier\n",
    "  from numpy import mean, std\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "\n",
    "  from tqdm import tqdm\n",
    "  \n",
    "  #set validation parameters\n",
    "  outer_cv = StratifiedKFold(n_splits=outer, random_state=seed)\n",
    "  inner_cv = StratifiedKFold(n_splits=inner, random_state=seed)\n",
    "  \n",
    "  \n",
    "  outer_scores = []\n",
    "  outer_estimates = []\n",
    "  outer_configurations = []\n",
    "  error_indices = []\n",
    "  \n",
    "  ground_truth = []\n",
    "  predicted_labels = []\n",
    "  predicted_probas = []\n",
    "\n",
    "  X, y = df.drop(columns=['classification']), df['classification']  \n",
    "\n",
    "  # Use the random grid to search for best hyperparameters\n",
    "  # configure the cross-validation procedure\n",
    "  cv_outer = StratifiedKFold(n_splits=outer, random_state=seed)\n",
    "  # enumerate splits\n",
    "  for train_ix, test_ix in tqdm(cv_outer.split(X,y), total=10):\n",
    "    # split data\n",
    "    # Get the training data\n",
    "    X_train, y_train = X.iloc[train_ix], y.iloc[train_ix]\n",
    "    # Get the validation data\n",
    "    X_test, y_test = X.iloc[test_ix], y.iloc[test_ix]\n",
    "\n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = StratifiedKFold(n_splits=inner, random_state=seed)\n",
    "    \n",
    "    \n",
    "    # define search\n",
    "    if search_cv=='random':\n",
    "      search = RandomizedSearchCV(\n",
    "        estimator = model(), \n",
    "        param_distributions = space, \n",
    "        n_iter = n_iter, # key parameter\n",
    "        cv = cv_inner,  # n_splits is key parameter\n",
    "    #     verbose=2, \n",
    "        random_state=seed, \n",
    "        scoring=scoring\n",
    "      )\n",
    "    else:\n",
    "      search = GridSearchCV(\n",
    "      model(), \n",
    "      space, \n",
    "      cv = cv_inner,  # n_splits is key parameter\n",
    "      scoring=scoring\n",
    "    )\n",
    "    # execute search\n",
    "    result = search.fit(X_train, y_train)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test).flatten()\n",
    "    # evaluate the model\n",
    "    acc = balanced_accuracy_score(y_test, yhat)\n",
    "    \n",
    "    # ROC METRICS \n",
    "    # Binarize the output\n",
    "    roc_y_train, roc_y_test = label_binarize(y_train, classes=[0, 1, 2,3]), label_binarize(y_test, classes=[0, 1, 2,3])\n",
    "    \n",
    "    try:\n",
    "      roc_params = result.best_params_.copy()\n",
    "      roc_params.pop('class_weight', None)\n",
    "      roc_params.pop('class_weights', None)\n",
    "      roc_params['class_weight'] = 'balanced'\n",
    "      roc_params['num_classes']=1\n",
    "      classifier = OneVsRestClassifier(model(**roc_params))\n",
    "      y_score = classifier.fit(X_train, roc_y_train).predict_proba(X_test)\n",
    "    except Exception as e:\n",
    "      try:\n",
    "        roc_params = result.best_params_.copy()\n",
    "        roc_params.pop('class_weight', None)\n",
    "        roc_params.pop('class_weights', None)\n",
    "        roc_params['class_weight'] = 'balanced'\n",
    "        classifier = OneVsRestClassifier(model(**roc_params))\n",
    "        y_score = classifier.fit(X_train, roc_y_train).predict_proba(X_test)\n",
    "      except:\n",
    "        roc_params = result.best_params_.copy()\n",
    "        roc_params.pop('class_weight', None)\n",
    "        roc_params.pop('class_weights', None)\n",
    "#         roc_params['class_weights'] = 'balanced'\n",
    "        classifier = OneVsRestClassifier(model(**roc_params))\n",
    "        y_score = classifier.fit(X_train, roc_y_train).predict_proba(X_test)\n",
    "      \n",
    "    predicted_probas.append(list(y_score))\n",
    "    \n",
    "    \n",
    "    #gather misclassified objects\n",
    "    y_test = np.asarray(y_test)\n",
    "    y_pred = best_model.predict(X_test).flatten()\n",
    "    misclassified = np.where(y_test != y_pred)\n",
    "    misclassified = list(X.iloc[test_ix].iloc[misclassified].index)\n",
    "    error_indices.append(misclassified)\n",
    "    ground_truth.append(list(y_test))\n",
    "    predicted_labels.append(list(y_pred))\n",
    "    \n",
    "    \n",
    "    # store the result\n",
    "    outer_scores.append(acc)\n",
    "    outer_estimates.append(result.best_score_)\n",
    "    outer_configurations.append(result.best_params_)\n",
    "    # report progress\n",
    "    print(f'>acc={acc:.2f}, est={result.best_score_:.3f}, cfg={result.best_params_}')\n",
    "    \n",
    "  error_indices = [item for sublist in error_indices for item in sublist]\n",
    "  \n",
    "  assert len(error_indices) == len(list(set(error_indices)))\n",
    "  final_data = pd.DataFrame({\n",
    "    'outer_scores': [outer_scores],\n",
    "    'outer_estimates': [outer_estimates],\n",
    "    'outer_configurations': [outer_configurations],\n",
    "    'error_indices': [error_indices],\n",
    "    'ground_truth': [[item for sublist in ground_truth for item in sublist]],\n",
    "    'predicted_labels': [[item for sublist in predicted_labels for item in sublist]],\n",
    "    'predicted_probas': [[list(item) for sublist in predicted_probas for item in sublist]]\n",
    "  })\n",
    "  \n",
    "  outer_average = np.array(final_data['outer_scores'][0]).mean()\n",
    "  final_data['outer_average'] = outer_average\n",
    "  \n",
    "  # summarize the estimated performance of the model\n",
    "  print(f\"Balanced Accuracy: {outer_average:.3f} ({std(np.array(final_data['outer_scores'][0])):.3f})\")\n",
    "  \n",
    "  return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_performance(df,model,outer,seed,tuned_acc):\n",
    "  \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Assess performance increase from hyper-parameter optimization\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "      Input ready in form of dataframe.\n",
    "        \n",
    "    model : machine learning model\n",
    "      Model in which to cross validate.\n",
    "      eg) CatboostRegressor, LightGBMRegressor\n",
    "      \n",
    "    seed : int, default=42\n",
    "      seed for reproducability\n",
    "      \n",
    "    tuned_mae : float\n",
    "      tuned model performance\n",
    "      \n",
    "  \"\"\"\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  from numpy import mean\n",
    "  from sklearn.metrics import balanced_accuracy_score\n",
    "  from numpy import std\n",
    "  X, y = df.drop(columns=['classification']), df['classification'] \n",
    "  #GET BASE PERFORMANCE\n",
    "  # Use the random grid to search for best hyperparameters\n",
    "  # configure the cross-validation procedure\n",
    "  cv_outer = StratifiedKFold(n_splits=outer, random_state=seed)\n",
    "  # enumerate splits\n",
    "  outer_results = list()\n",
    "  counter=0\n",
    "  for train_ix, test_ix in cv_outer.split(X,y):\n",
    "    # split data\n",
    "    # Get the training data\n",
    "    X_train, y_train = X.iloc[train_ix], y.iloc[train_ix]\n",
    "    # Get the validation data\n",
    "    X_test, y_test = X.iloc[test_ix], y.iloc[test_ix]\n",
    "\n",
    "    # define the model\n",
    "    model = model\n",
    "\n",
    "    # execute search\n",
    "    result = model.fit(X_train, y_train)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = result.predict(X_test)\n",
    "    # evaluate the model\n",
    "    acc = balanced_accuracy_score(y_test, yhat)\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print(f'>acc={acc:.2f}')\n",
    "    \n",
    "  # summarize the estimated performance of the model\n",
    "  base_acc = mean(outer_results)\n",
    "  print(f'Tuned Accuracy: {tuned_acc:.3f}')\n",
    "  print(f'Base Accuracy: {base_acc:.3f}')\n",
    "\n",
    "  print(f\"Improvement of: {(100 * (tuned_acc - base_acc) / base_acc):.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(y_true,predicted_probas,n_classes=4):\n",
    "  \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Plot ROC curves\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : list\n",
    "      Ground truth labels\n",
    "        \n",
    "    predicted_probas : list\n",
    "      Confidence scores of model\n",
    "      \n",
    "    num_classes : int, default=4\n",
    "      number of classes for classification.\n",
    "      eg) 4, 6, 8\n",
    "      \n",
    "  \"\"\"\n",
    "  from scipy import interp\n",
    "  from sklearn.metrics import roc_curve, auc\n",
    "  from itertools import cycle\n",
    "  from sklearn.preprocessing import label_binarize\n",
    "  import numpy as np\n",
    "  \n",
    "  import matplotlib.pyplot as plt\n",
    "  %matplotlib inline\n",
    "\n",
    "  y_true = label_binarize(y_true, classes=[0, 1, 2,3])\n",
    "\n",
    "  adjusted = []\n",
    "  for val in predicted_probas:\n",
    "    adjusted.append(list(val))\n",
    "  adjusted=np.array(adjusted)\n",
    "  predicted_probas = adjusted\n",
    "\n",
    "  fpr = dict()\n",
    "  tpr = dict()\n",
    "  roc_auc = dict()\n",
    "  for i in range(n_classes):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_true[:, i], predicted_probas[:, i])\n",
    "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "  colors = cycle(['blue', 'red', 'green', 'yellow'])\n",
    "\n",
    "  # First aggregate all false positive rates\n",
    "  all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "  # Then interpolate all ROC curves at this points\n",
    "  mean_tpr = np.zeros_like(all_fpr)\n",
    "  for i in range(n_classes):\n",
    "      mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "  # Finally average it and compute AUC\n",
    "  mean_tpr /= n_classes\n",
    "\n",
    "  fpr[\"macro\"] = all_fpr\n",
    "  tpr[\"macro\"] = mean_tpr\n",
    "  roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "  plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "           label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                 ''.format(roc_auc[\"macro\"]),\n",
    "           linewidth=2)\n",
    "\n",
    "  for i, color in zip(range(n_classes), colors):\n",
    "      plt.plot(fpr[i], tpr[i], color=color, lw=1,\n",
    "               label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "               ''.format(i, roc_auc[i]))\n",
    "\n",
    "  plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "  plt.xlim([-0.05, 1.0])\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('Receiver operating characteristic for multi-class data')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_confused(y_true,y_pred,normalize=True):\n",
    "  \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Display confusion matrix\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : list\n",
    "      Ground truth labels\n",
    "        \n",
    "    y_pred : list\n",
    "      Predicted classes\n",
    "      \n",
    "    normalize : boolean, default=True\n",
    "      Display normalized confusion matrix\n",
    "      \n",
    "  \"\"\"\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "  %matplotlib inline\n",
    "  sns.set()\n",
    "  import scikitplot as skplt\n",
    "  import numpy as np\n",
    "\n",
    "  y_true = list(y_true)\n",
    "  y_pred = list(y_pred)\n",
    "  skplt.metrics.plot_confusion_matrix(\n",
    "          y_true,\n",
    "          y_pred,\n",
    "         title=\"Confusion matrix\", normalize=True)\n",
    "\n",
    "  #FIX DISPLAY OF RESULTS\n",
    "  plt.xlim(-0.5, len(np.unique(y_true))-0.5)\n",
    "  plt.ylim(len(np.unique(y_true))-0.5, -0.5)\n",
    "  #   plt.ylim(-0.5, 4.5)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightsArr(values):\n",
    "  weights_arr = []\n",
    "  y = values.values.tolist()\n",
    "  num_classes = len(values.unique().tolist())\n",
    "  for i in range(num_classes):\n",
    "    percentage = y.count(i)/len(y)\n",
    "    result = 1 - percentage\n",
    "    weights_arr.append(result)\n",
    "  return weights_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightsDict(values):\n",
    "  weights={}\n",
    "  y = values.values.tolist()\n",
    "  num_classes = len(values.unique().tolist())\n",
    "  for i in range(num_classes):\n",
    "    percentage = y.count(i)/len(y)\n",
    "    result = 1 - percentage\n",
    "    weights[i]=result\n",
    "  return weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
