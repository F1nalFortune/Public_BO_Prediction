{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_val(df,model,space, n_iter,scoring,search_cv='random',outer=10,inner=3,seed=42):\n",
    "  \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Creates nested folds manually.\n",
    "    Returns dataframe  of validation data\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "      Input ready in form of dataframe.\n",
    "        \n",
    "    model : machine learning model\n",
    "      Model in which to cross validate.\n",
    "      eg) CatboostRegressor, LightGBMRegressor\n",
    "      \n",
    "    space : dictionary of lists\n",
    "      Search space for hyper parameter optimization.\n",
    "      eg) {\n",
    "            'num_leaves': [300,400,500], \n",
    "            'max_bin': [175,255,510],\n",
    "            'num_iterations': [700,800,900], \n",
    "            'learning_rate':[0.05,0.1,0.15],\n",
    "            'boosting_type': ['gbdt', 'dart'],\n",
    "            'max_depth': [-1, 5, 10]\n",
    "          }\n",
    "      \n",
    "    n_iter : number of parameter settings sampled\n",
    "      n_iter trades off runtime vs quality of the solution.\n",
    "      \n",
    "    scoring : string\n",
    "      score method for evaluating model\n",
    "      \n",
    "    search_cv : string, default='random'\n",
    "      hyper-parameter optimization approach. Random and grid search options\n",
    "      eg)'random' or 'grid'\n",
    "      \n",
    "    outer : int, default=10\n",
    "      number of outer folds\n",
    "      \n",
    "    inner : int, default=3\n",
    "      number of inner folds\n",
    "      \n",
    "    seed : int, default=42\n",
    "      seed for reproducability\n",
    "      \n",
    "    \"\"\"\n",
    "  from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "  from sklearn.metrics import mean_absolute_error\n",
    "  from numpy import mean, std\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  from tqdm import tqdm\n",
    "  def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    import numpy as np\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "  outer_scores = []\n",
    "  outer_mape = []\n",
    "  outer_estimates = []\n",
    "  outer_configurations = []\n",
    "  error_indices = []\n",
    "  \n",
    "  ground_truth = []\n",
    "  predicted_labels = []\n",
    "\n",
    "  X, y = df.drop(columns=['box_office']), df['box_office']  \n",
    "\n",
    "  # Use the random grid to search for best hyperparameters\n",
    "  # configure the cross-validation procedure\n",
    "  cv_outer = KFold(n_splits=outer, random_state=seed)\n",
    "  cv_inner = KFold(n_splits=inner, random_state=seed)\n",
    "  # enumerate splits\n",
    "  for train_ix, test_ix in tqdm(cv_outer.split(X,y), total=10):\n",
    "    # split data\n",
    "    # Get the training data\n",
    "    X_train, y_train = X.iloc[train_ix], y.iloc[train_ix]\n",
    "    # Get the validation data\n",
    "    X_test, y_test = X.iloc[test_ix], y.iloc[test_ix]\n",
    "    \n",
    "    # define search\n",
    "    if search_cv=='random':\n",
    "      search = RandomizedSearchCV(\n",
    "        estimator = model(), \n",
    "        param_distributions = space, \n",
    "        n_iter = n_iter,\n",
    "        cv = cv_inner,\n",
    "        random_state=seed, \n",
    "        scoring=scoring\n",
    "      )\n",
    "    else:\n",
    "      search = GridSearchCV(\n",
    "      model(), \n",
    "      space, \n",
    "      cv = cv_inner,\n",
    "      scoring=scoring\n",
    "    )\n",
    "    # execute search\n",
    "    result = search.fit(X_train, y_train)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test).flatten()\n",
    "    # evaluate the model\n",
    "    mae = mean_absolute_error(np.expm1(y_test), np.expm1(yhat))\n",
    "    mape = mean_absolute_percentage_error(np.expm1(y_test), np.expm1(yhat))\n",
    "    \n",
    "    #gather misclassified objects\n",
    "    y_test = np.asarray(y_test)\n",
    "    y_pred = best_model.predict(X_test).flatten()\n",
    "    misclassified = np.where(y_test != y_pred)\n",
    "    misclassified = list(X.iloc[test_ix].iloc[misclassified].index)\n",
    "    error_indices.append(misclassified)\n",
    "    ground_truth.append(list(y_test))\n",
    "    predicted_labels.append(list(y_pred))\n",
    "    \n",
    "    # store the result\n",
    "    outer_scores.append(mae)\n",
    "    outer_mape.append(mape)\n",
    "    outer_estimates.append(result.best_score_)\n",
    "    outer_configurations.append(result.best_params_)\n",
    "    # report progress\n",
    "    print(f'>$USD={mae:.3f}, mape={mape:.3f} mae={mae:.3f},est={result.best_score_:.3f}, cfg={result.best_params_}')\n",
    "    \n",
    "  error_indices = [item for sublist in error_indices for item in sublist]\n",
    "  \n",
    "  final_data = pd.DataFrame({\n",
    "    'outer_mae': [outer_scores],\n",
    "    'outer_mape': [outer_mape],\n",
    "    'outer_estimates': [outer_estimates],\n",
    "    'outer_configurations': [outer_configurations],\n",
    "    'error_indices': [error_indices],\n",
    "    'ground_truth': [[item for sublist in ground_truth for item in sublist]],\n",
    "    'predicted_labels': [[item for sublist in predicted_labels for item in sublist]]\n",
    "  })\n",
    "  \n",
    "  outer_average = np.array(final_data['outer_mae'][0]).mean()\n",
    "  final_data['outer_average'] = outer_average\n",
    "  \n",
    "  # summarize the estimated performance of the model\n",
    "  print(f\"Mean Absolute Error: {outer_average:.3f} ({std(np.array(final_data['outer_mae'][0])):.3f})\")\n",
    "  \n",
    "  return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_performance(df,model,outer,seed,tuned_mae):\n",
    "  \"\"\"\n",
    "    Objective\n",
    "    ---------\n",
    "    Assess performance increase from hyper-parameter optimization\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "      Input ready in form of dataframe.\n",
    "        \n",
    "    model : machine learning model\n",
    "      Model in which to cross validate.\n",
    "      eg) CatboostRegressor, LightGBMRegressor\n",
    "      \n",
    "    seed : int, default=42\n",
    "      seed for reproducability\n",
    "      \n",
    "    tuned_mae : float\n",
    "      tuned model performance\n",
    "      \n",
    "  \"\"\"\n",
    "  from sklearn.model_selection import KFold\n",
    "  from numpy import mean\n",
    "  from sklearn.metrics import mean_absolute_error\n",
    "  from numpy import std\n",
    "  \n",
    "  X, y = df.drop(columns=['box_office']), df['box_office'] \n",
    "  #GET BASE PERFORMANCE\n",
    "  # Use the random grid to search for best hyperparameters\n",
    "  # configure the cross-validation procedure\n",
    "  cv_outer = KFold(n_splits=outer, random_state=seed)\n",
    "  outer_results = list()\n",
    "  counter=0\n",
    "  for train_ix, test_ix in cv_outer.split(X,y):\n",
    "    # split data\n",
    "    # Get the training data\n",
    "    X_train, y_train = X.iloc[train_ix], y.iloc[train_ix]\n",
    "    # Get the validation data\n",
    "    X_test, y_test = X.iloc[test_ix], y.iloc[test_ix]\n",
    "\n",
    "    # define the model\n",
    "    model = model\n",
    "\n",
    "    # execute search\n",
    "    result = model.fit(X_train, y_train)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = result.predict(X_test)\n",
    "    # evaluate the model\n",
    "    mae = mean_absolute_error(y_test, yhat)\n",
    "    # store the result\n",
    "    outer_results.append(mae)\n",
    "    # report progress\n",
    "    print(f'>mae={mae:.2f}')\n",
    "    \n",
    "  # summarize the estimated performance of the model\n",
    "  base_mae = mean(outer_results)\n",
    "  print(f'Tuned MAE: {tuned_mae:.3f}')\n",
    "  print(f'Base MAE: {base_mae:.3f}')\n",
    "\n",
    "  print(f\"Improvement of: { 100 * (tuned_mae - base_mae) / base_mae}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    import numpy as np\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
